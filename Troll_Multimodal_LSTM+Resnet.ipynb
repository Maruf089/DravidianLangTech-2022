{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iNLvRyHd4gv",
    "outputId": "610d2363-1293-43d4-f04c-73b6405f82b4"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive \n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gDUDDv0tSgSF"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XXIPVWuySf-E"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gdGiYYl_eHUp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "#Importing all the libraries needed\n",
    "import keras\n",
    "import tensorflow\n",
    "import h5py\n",
    "from keras import optimizers, preprocessing, Input\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.core import Reshape, Dropout\n",
    "from keras.utils.vis_utils import  plot_model\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling3D\n",
    "from keras import regularizers\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Embedding, LSTM, multiply\n",
    "from PIL import Image, ImageFile\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "qtytbtlOecG1",
    "outputId": "5d40a105-05ad-4c99-8492-18cf4d7f029d"
   },
   "outputs": [],
   "source": [
    "# text pre-processing functions\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\nBULLET::::', ' ')\n",
    "    text = text.replace('BULLET::::-', ' ')\n",
    "    text = text.replace('BULLET::::', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\n\\n', ' ')\n",
    "    text = text.replace(r',', '')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace(' - ', '')\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('Page', ' ')\n",
    "    text = text.replace(':', ' ')\n",
    "    text = text.replace(';', ' ')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\d+.', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_tamil_stop_words():\n",
    "    swdf1 = pd.read_csv(\"D:\\\\My Research\\SHared Task\\\\Stopwords\\\\TamilNLP_TamilStopWords.txt\",  header=None) \n",
    "    swdf2 = pd.read_csv(\"D:\\\\My Research\\SHared Task\\\\Stopwords\\\\TamilNLP_TamilStopWords.txt\",  header=None) \n",
    "    sw1 = swdf1[0].tolist()\n",
    "    sw2 = swdf2[0].tolist()\n",
    "    tamil_stop_words = list(set(sw1 + sw2))\n",
    "    return tamil_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dtq_Cdb9h70l"
   },
   "outputs": [],
   "source": [
    "tamil_stop_words = get_tamil_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dx-WTQdph7t7"
   },
   "outputs": [],
   "source": [
    "#LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F4UKDG3Du0D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fiX6sB3wI1ft"
   },
   "outputs": [],
   "source": [
    "Training_path = 'D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\train_captions_final.csv'\n",
    "Testing_path = 'D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\test_captions.csv'\n",
    "Validation_path = 'D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\test_captions_final.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceDZw1gALg1k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHqJ_OYaLh6V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsV8nXXojSuL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gp3KktTPKHR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rcw-khRABFfz"
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "  #Opening file\n",
    "    with open(file_name,'r', encoding=\"utf8\",newline='\\n', errors='ignore') as f:\n",
    "      #Creating empty set and dictonary for vocab and word respectively\n",
    "        word_vocab = set() \n",
    "        word2vector = {}\n",
    "        #Iterating over each line of file\n",
    "        for line in f:\n",
    "            #Spliting lines\n",
    "            line_ = line.strip() \n",
    "            #Splitting words\n",
    "            words_Vec = line_.split()            \n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IzJx3ZlU1BTS"
   },
   "outputs": [],
   "source": [
    "def encode_label(DataFrame, Label_col):\n",
    "    t_y = DataFrame[Label_col].values\n",
    "    Encoder = LabelEncoder()\n",
    "    y = Encoder.fit_transform(t_y)\n",
    "    DataFrame[Label_col] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RlFYJiAVBOGW"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(Training_path,Validation_path, Testing_path):\n",
    "    # function to preprocess input\n",
    "    training_DF = pd.read_csv(Training_path, sep = ',')\n",
    "    validation_DF = pd.read_csv(Validation_path, sep = ',')\n",
    "    testing_DF = pd.read_csv(Testing_path, sep = ',')\n",
    "\n",
    "    # encoding all the labels \n",
    "    encode_label(testing_DF,'label')\n",
    "    encode_label(training_DF, 'label')\n",
    "    encode_label(validation_DF, 'label')\n",
    "    \n",
    "    training_DF['sentence'] = training_DF['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (tamil_stop_words)]))      \n",
    "    validation_DF['sentence'] = validation_DF['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (tamil_stop_words)]))\n",
    "    testing_DF['sentence'] = testing_DF['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (tamil_stop_words)]))\n",
    "\n",
    "    # Processing the text\n",
    "    training_DF['sentence'] = training_DF['sentence'].apply(clean_text)\n",
    "    testing_DF['sentence'] = testing_DF['sentence'].apply(clean_text)\n",
    "    validation_DF['sentence'] = validation_DF['sentence'].apply(clean_text)\n",
    "\n",
    "    return training_DF, testing_DF, validation_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y489TDKfBN_9"
   },
   "outputs": [],
   "source": [
    "#splitting data into train, test and validation\n",
    "training_df, testing_df, validation_df = preprocess_text(Training_path,Validation_path, Testing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYzH-z_-BN7r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsffRT_XFmru"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "m650MxnvFmnt"
   },
   "outputs": [],
   "source": [
    "maxlen = 1000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZNI_-0sFmjo",
    "outputId": "50574ed4-0cad-4f1b-c203-5b54adfe12a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#model with adam optimizer\n",
    "adam = tensorflow.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = tensorflow.keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adadelta = tensorflow.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UyqOUAc3FmgG"
   },
   "outputs": [],
   "source": [
    "# Vectorising text\n",
    "# process the whole observation into single list\n",
    "train_text_list= list(training_df['sentence'])\n",
    "test_text_list = list(testing_df['sentence'])\n",
    "val_text_list = list(validation_df['sentence'])\n",
    "\n",
    "# Creating vectors for train, test, validation\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(train_text_list)\n",
    "sequences_train = tokenizer.texts_to_sequences(train_text_list)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_text_list)\n",
    "sequences_val = tokenizer.texts_to_sequences(val_text_list)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "x_val = preprocessing.sequence.pad_sequences(sequences_val, maxlen=maxlen)\n",
    "\n",
    "\n",
    "y_test = testing_df['label']\n",
    "y_train = training_df['label']\n",
    "y_val = validation_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HGO5_9OHFmU4"
   },
   "outputs": [],
   "source": [
    "def img_text_generator(files, padded_seq, y, batch_size=None):\n",
    "    \"\"\"\n",
    "        padded_seq: vectorized padded text sequence \n",
    "        y: label of the text\n",
    "        batch_size: Number of observations to be selected at a time\n",
    "        \n",
    "        return: generator object of text data\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_idxs = np.random.choice(a = list(range(len(padded_seq))), size=batch_size) #Selecting the random batch indexes    \n",
    "        batch_input_txt = [] # Initializing batch input text\n",
    "        batch_input_img = [] # Initializing batch input image\n",
    "        batch_output = [] # Initializing batch output\n",
    "        \n",
    "        # Traversing through the batch indexes\n",
    "        for batch_idx in batch_idxs:\n",
    "            input_txt = padded_seq[batch_idx] # selecting padded sequences from the batch\n",
    "            output = y[batch_idx] # Selecting label  \n",
    "            input_img = get_input(files[batch_idx])\n",
    "            input_img = process_input(input_img)\n",
    "            batch_input_txt.append(input_txt) # Appending the input (text vector)\n",
    "            batch_input_img.append(input_img[0])\n",
    "            batch_output.append(output) # Appending the label\n",
    "        \n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x1 = np.array( batch_input_img )\n",
    "        batch_x2 = np.array( batch_input_txt )\n",
    "        batch_y = np.array( batch_output )\n",
    "        yield ([batch_x1, batch_x2], batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "as0XM0fpFmQv"
   },
   "outputs": [],
   "source": [
    "def text_generator(padded_seq, y, batch_size=None):\n",
    "    \"\"\"\n",
    "        padded_seq: vectorized padded text sequence \n",
    "        y: label of the text\n",
    "        batch_size: Number of observations to be selected at a time\n",
    "        \n",
    "        return: generator object of text data\n",
    "    \"\"\"\n",
    "    idxs = list(range(len(y)))\n",
    "    idx = 0\n",
    "    while True:\n",
    "        batch_idxs = idxs[idx:idx+batch_size]\n",
    "        idx = idx + batch_size\n",
    "#         batch_idxs = np.random.choice(a = list(range(len(padded_seq))), size=batch_size) #Selecting the random batch indexes    \n",
    "        batch_input = [] # Initializing batch input\n",
    "        batch_output = [] # Initializing batch output\n",
    "        \n",
    "        # Traversing through the batch indexes\n",
    "        for batch_idx in batch_idxs:\n",
    "            input = padded_seq[batch_idx] # selecting padded sequences from the batch\n",
    "            output = y[batch_idx] # Selecting label            \n",
    "            batch_input.append(input) # Appending the input (text vector)\n",
    "            batch_output.append(output) # Appending the label\n",
    "        \n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x = np.array( batch_input )\n",
    "        batch_y = np.array( batch_output )\n",
    "        if len(batch_x) < batch_size:\n",
    "            idx = 0\n",
    "        else:             \n",
    "            yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "o88abWvKFmNI"
   },
   "outputs": [],
   "source": [
    "def get_input(path):\n",
    "    # Loading image from given path\n",
    "    # and resizing it to 224*224*3 format\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    img = image.load_img(path, target_size=(224,224))    \n",
    "    return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Km7uo0M2FmJn"
   },
   "outputs": [],
   "source": [
    "def get_output(path,label_file=None):\n",
    "    # Spliting the path and take out the image id    \n",
    "    filename = path.split('/')[-1]\n",
    "    # Taking list of labels\n",
    "    labels = list(label_file[label_file['image_name'] == filename]['label'].values)\n",
    "  #  for duplicate selecting labels\n",
    "    if len(labels) <= 2:\n",
    "        label = labels[0]\n",
    "    elif len(labels) > 2:\n",
    "        uni_label = list(set(labels))\n",
    "        count_label = [labels.count(lab) for lab in uni_label]\n",
    "        lab_idx = count_label.index(max(count_label))\n",
    "        label = uni_label[lab_idx]\n",
    "    #label = labels[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WQnragxFFmGI"
   },
   "outputs": [],
   "source": [
    "def process_input(img):\n",
    "    # Converting image to array    \n",
    "    img_data = image.img_to_array(img)\n",
    "    # Adding one more dimension to array    \n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    #     \n",
    "    img_data = preprocess_input(img_data)\n",
    "    return(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SnB8Uh2HIvE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yfVC9no4HIrc"
   },
   "outputs": [],
   "source": [
    " \n",
    "# Creating train, test, val, generator for text\n",
    "txt_gen_train = text_generator(x_train, y_train, batch_size=32)\n",
    "txt_gen_test = text_generator(x_test, y_test, batch_size=1)\n",
    "txt_gen_val = text_generator(x_val, y_val, batch_size=1)\n",
    "\n",
    "# text_generator() Return a tuple of (padded sequences array,label array) to feed the network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPFEWt-JHIoL",
    "outputId": "83645e9f-6604-4787-9c95-71be9e1cd61b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bWyUpplHIko"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNLbdkSft3Nt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R0Uq3OM-t3Ky"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/implementing-neural-machine-translation-using-keras-8312e4844eb8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nOpo21xvt3Du"
   },
   "outputs": [],
   "source": [
    "# nice reading blog for this implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IGops78t-YH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# model_ft = KeyedVectors.load_word2vec_format(\"D:\\\\My Research\\\\SHared Task\\\\cc.ta.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #embedding weight matrix\n",
    "\n",
    "# embedding_matrix_1 = np.zeros((num_tokens + 1, EMBEDDING_DIM))\n",
    "\n",
    "# count = 0 ;\n",
    "\n",
    "# for word,i in word_index.items():\n",
    "\n",
    "#     try:\n",
    "#         embedding_matrix_1[i] = np.array(model_ft[word])\n",
    "#         count = count + 1;  \n",
    "#     except Exception:\n",
    "#         ;\n",
    "\n",
    "# print(count);\n",
    "# print(embedding_matrix_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 32)          320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.6353 - f1_m: 0.7274\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.42136, saving model to D:\\My Research\\SHared Task\\Troll Meme\\Tamil_troll_memes\\tamil_memes_modified_Nusrat\\weights_best_resnest_txt.hdf5\n",
      "51/51 [==============================] - 119s 2s/step - loss: 0.6353 - f1_m: 0.7274 - val_loss: 0.6139 - val_f1_m: 0.4214\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.4694 - f1_m: 0.8113\n",
      "Epoch 00002: val_f1_m did not improve from 0.42136\n",
      "51/51 [==============================] - 129s 3s/step - loss: 0.4694 - f1_m: 0.8113 - val_loss: 0.5174 - val_f1_m: 0.4062\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.3341 - f1_m: 0.8919\n",
      "Epoch 00003: val_f1_m did not improve from 0.42136\n",
      "51/51 [==============================] - 114s 2s/step - loss: 0.3341 - f1_m: 0.8919 - val_loss: 0.4695 - val_f1_m: 0.3686\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2378 - f1_m: 0.9166\n",
      "Epoch 00004: val_f1_m did not improve from 0.42136\n",
      "51/51 [==============================] - 124s 2s/step - loss: 0.2378 - f1_m: 0.9166 - val_loss: 0.4333 - val_f1_m: 0.4194\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1916 - f1_m: 0.9333\n",
      "Epoch 00005: val_f1_m improved from 0.42136 to 0.42744, saving model to D:\\My Research\\SHared Task\\Troll Meme\\Tamil_troll_memes\\tamil_memes_modified_Nusrat\\weights_best_resnest_txt.hdf5\n",
      "51/51 [==============================] - 112s 2s/step - loss: 0.1916 - f1_m: 0.9333 - val_loss: 0.4338 - val_f1_m: 0.4274\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1704 - f1_m: 0.9360\n",
      "Epoch 00006: val_f1_m did not improve from 0.42744\n",
      "51/51 [==============================] - 120s 2s/step - loss: 0.1704 - f1_m: 0.9360 - val_loss: 0.4484 - val_f1_m: 0.4216\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1454 - f1_m: 0.9479\n",
      "Epoch 00007: val_f1_m did not improve from 0.42744\n",
      "51/51 [==============================] - 117s 2s/step - loss: 0.1454 - f1_m: 0.9479 - val_loss: 0.4711 - val_f1_m: 0.4133\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1294 - f1_m: 0.9518\n",
      "Epoch 00008: val_f1_m did not improve from 0.42744\n",
      "51/51 [==============================] - 125s 2s/step - loss: 0.1294 - f1_m: 0.9518 - val_loss: 0.5424 - val_f1_m: 0.4070\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1240 - f1_m: 0.9560\n",
      "Epoch 00009: val_f1_m did not improve from 0.42744\n",
      "51/51 [==============================] - 112s 2s/step - loss: 0.1240 - f1_m: 0.9560 - val_loss: 0.5422 - val_f1_m: 0.4100\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1138 - f1_m: 0.9539\n",
      "Epoch 00010: val_f1_m did not improve from 0.42744\n",
      "51/51 [==============================] - 105s 2s/step - loss: 0.1138 - f1_m: 0.9539 - val_loss: 0.5444 - val_f1_m: 0.4121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20dbac1f400>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# LSTM \n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_vecor_length, input_length=1000))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_m])\n",
    "print(model.summary())\n",
    "\n",
    "subdir = \"D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\\";\n",
    "filepath = os.path.join(subdir,\"weights_best_resnest_txt.hdf5\") \n",
    "checkpoint = ModelCheckpoint(filepath, monitor= \"val_f1_m\" , verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32,verbose = 1 , callbacks = callbacks_list, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting the labels for the test set\n",
    "y_pred_txt = (model.predict_generator(txt_gen_test, steps=663))\n",
    "y_pred_txt = np.round(list(itertools.chain(*y_pred_txt)))\n",
    "\n",
    "y_pred_txt\n",
    "\n",
    "\n",
    "\n",
    "#true labels\n",
    "#y_true = y_test.values\n",
    "#y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_txt = [ int(x) for x in y_pred_txt ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text_df = pd.read_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\submit1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0;\n",
    "for i in y_pred_txt:\n",
    "    if i == 1:\n",
    "      only_text_df['label'][count] = i;\n",
    "    count = count + 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_text_df['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_img_0.jpg</td>\n",
       "      <td>sugarkaga nadandhava vida figuregaga nadandhav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_img_1.jpg</td>\n",
       "      <td>i have come for my stones      stones thaane.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_img_2.jpg</td>\n",
       "      <td>\"special porotta\" nu pottuierukke spacial kum ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_img_3.jpg</td>\n",
       "      <td>*we : amma .. cooker 3 whistle vanthuchu off p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_img_4.jpg</td>\n",
       "      <td>creating whatsapp group - 1st day vaanga ji.. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name                                           sentence label\n",
       "0  test_img_0.jpg  sugarkaga nadandhava vida figuregaga nadandhav...     1\n",
       "1  test_img_1.jpg  i have come for my stones      stones thaane.....     1\n",
       "2  test_img_2.jpg  \"special porotta\" nu pottuierukke spacial kum ...     1\n",
       "3  test_img_3.jpg  *we : amma .. cooker 3 whistle vanthuchu off p...     1\n",
       "4  test_img_4.jpg  creating whatsapp group - 1st day vaanga ji.. ...     1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0;\n",
    "for i in y_pred_txt:\n",
    "    if i == 1:\n",
    "      only_text_df['label'][count] = \"troll\";\n",
    "    else:\n",
    "        only_text_df['label'][count] = \"not_troll\";\n",
    "    count = count + 1;\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_img_0.jpg</td>\n",
       "      <td>sugarkaga nadandhava vida figuregaga nadandhav...</td>\n",
       "      <td>troll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_img_1.jpg</td>\n",
       "      <td>i have come for my stones      stones thaane.....</td>\n",
       "      <td>troll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_img_2.jpg</td>\n",
       "      <td>\"special porotta\" nu pottuierukke spacial kum ...</td>\n",
       "      <td>troll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_img_3.jpg</td>\n",
       "      <td>*we : amma .. cooker 3 whistle vanthuchu off p...</td>\n",
       "      <td>troll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_img_4.jpg</td>\n",
       "      <td>creating whatsapp group - 1st day vaanga ji.. ...</td>\n",
       "      <td>troll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name                                           sentence  label\n",
       "0  test_img_0.jpg  sugarkaga nadandhava vida figuregaga nadandhav...  troll\n",
       "1  test_img_1.jpg  i have come for my stones      stones thaane.....  troll\n",
       "2  test_img_2.jpg  \"special porotta\" nu pottuierukke spacial kum ...  troll\n",
       "3  test_img_3.jpg  *we : amma .. cooker 3 whistle vanthuchu off p...  troll\n",
       "4  test_img_4.jpg  creating whatsapp group - 1st day vaanga ji.. ...  troll"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text_df.to_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\my_submit1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 4s 148ms/step - loss: 1.7098 - f1_m: 0.6974\n",
      "Accuracy : 69.74%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_vecor_length, input_length=1000))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.load_weights(filepath)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])\n",
    "scores = model.evaluate(x_test, y_test, verbose=1,batch_size = 32)\n",
    "print(\"Accuracy : %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1000, 32)          320000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1000, 32)          3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 500, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 376,405\n",
      "Trainable params: 376,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.6300 - f1_m: 0.7367\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.41115, saving model to D:\\My Research\\SHared Task\\Troll Meme\\Tamil_troll_memes\\tamil_memes_modified_Nusrat\\weights_best_cnn.hdf5\n",
      "51/51 [==============================] - 25s 438ms/step - loss: 0.6300 - f1_m: 0.7367 - val_loss: 0.5981 - val_f1_m: 0.4112\n",
      "Epoch 2/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.4230 - f1_m: 0.8361\n",
      "Epoch 00002: val_f1_m improved from 0.41115 to 0.42144, saving model to D:\\My Research\\SHared Task\\Troll Meme\\Tamil_troll_memes\\tamil_memes_modified_Nusrat\\weights_best_cnn.hdf5\n",
      "51/51 [==============================] - 22s 431ms/step - loss: 0.4230 - f1_m: 0.8361 - val_loss: 0.4830 - val_f1_m: 0.4214\n",
      "Epoch 3/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2632 - f1_m: 0.9049\n",
      "Epoch 00003: val_f1_m did not improve from 0.42144\n",
      "51/51 [==============================] - 22s 428ms/step - loss: 0.2632 - f1_m: 0.9049 - val_loss: 0.4466 - val_f1_m: 0.4007\n",
      "Epoch 4/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2365 - f1_m: 0.9303\n",
      "Epoch 00004: val_f1_m did not improve from 0.42144\n",
      "51/51 [==============================] - 22s 429ms/step - loss: 0.2365 - f1_m: 0.9303 - val_loss: 0.4822 - val_f1_m: 0.4051\n",
      "Epoch 5/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1502 - f1_m: 0.9440\n",
      "Epoch 00005: val_f1_m did not improve from 0.42144\n",
      "51/51 [==============================] - 22s 430ms/step - loss: 0.1502 - f1_m: 0.9440 - val_loss: 0.4907 - val_f1_m: 0.4144\n",
      "Epoch 6/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2036 - f1_m: 0.9350\n",
      "Epoch 00006: val_f1_m did not improve from 0.42144\n",
      "51/51 [==============================] - 22s 432ms/step - loss: 0.2036 - f1_m: 0.9350 - val_loss: 0.4657 - val_f1_m: 0.4195\n",
      "Epoch 7/7\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.1333 - f1_m: 0.9512\n",
      "Epoch 00007: val_f1_m did not improve from 0.42144\n",
      "51/51 [==============================] - 23s 444ms/step - loss: 0.1333 - f1_m: 0.9512 - val_loss: 0.5303 - val_f1_m: 0.4143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20dbf705320>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN+LSTM create the model\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_vecor_length, input_length=1000))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])\n",
    "print(model.summary())\n",
    "\n",
    "subdir = \"D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\\";\n",
    "filepath = os.path.join(subdir,\"weights_best_cnn.hdf5\") \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1_m', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=7, batch_size=32,verbose = 1 , callbacks = callbacks_list, validation_data=(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os         # Creating a directory\n",
    "model.save('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\CNN_LSTM_Text_model.h5')   # Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting the labels for the test set\n",
    "y_pred_txt = (model.predict_generator(txt_gen_test, steps=663))\n",
    "y_pred_txt = np.round(list(itertools.chain(*y_pred_txt)))\n",
    "\n",
    "y_pred_txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_txt = [ int(x) for x in y_pred_txt ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text_df = pd.read_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\submit1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0;\n",
    "for i in y_pred_txt:\n",
    "    if i == 1:\n",
    "      only_text_df['label'][count] = \"troll\";\n",
    "    else:\n",
    "        only_text_df['label'][count] = \"not_troll\";\n",
    "    count = count + 1;\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text_df.to_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\my_submit2_txt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 1000, 32)          320000    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1000, 32)          3104      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 500, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 376,405\n",
      "Trainable params: 376,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 68.83%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, embedding_vecor_length, input_length=1000))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m])\n",
    "print(model.summary())\n",
    "model.load_weights(filepath)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "9uu9qnRJWiE7"
   },
   "outputs": [],
   "source": [
    "#Image Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNet, VGG16, xception, ResNet50\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the libraries needed\n",
    "import keras\n",
    "import tensorflow\n",
    "import h5py\n",
    "from keras import optimizers, preprocessing, Input\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.core import Reshape, Dropout\n",
    "from keras.utils.vis_utils import  plot_model\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling3D\n",
    "from keras import regularizers\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Embedding, LSTM, multiply\n",
    "from PIL import Image, ImageFile\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#model with adam optimizer\n",
    "adam = tensorflow.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = tensorflow.keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adadelta = tensorflow.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_troll = os.path.realpath(\"D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\Troll_meme\\\\train\\\\troll\")\n",
    "path_NOT = os.path.realpath(\"D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\Troll_meme\\\\train\\\\Not_troll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\Troll_meme\\\\train\\\\troll'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_troll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_img = os.listdir(path_troll)\n",
    "dir_troll = [os.path.join(path_troll, filename) for filename in troll_img]\n",
    "Not_troll_img = os.listdir(path_NOT)\n",
    "dir_not = [os.path.join(path_NOT, filename) for filename in Not_troll_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of troll images:  951 \n",
      " # of not troll images:  666\n"
     ]
    }
   ],
   "source": [
    "print(\"# of troll images: \", len(troll_img), '\\n',\n",
    "      \"# of not troll images: \", len(Not_troll_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_image(file):\n",
    "    img_path = ''\n",
    "    img = image.load_img(img_path + file, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return keras.applications.vgg16.preprocess_input(img_array_expanded_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_model(base_model):\n",
    "  #freezing all the trainable layers\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "  #create output layer\n",
    "  x = base_model.output\n",
    "  #pooling layer before the output\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=ResNet50(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(1,activation='sigmoid')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model = Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv1_pad\n",
      "2 conv1_conv\n",
      "3 conv1_bn\n",
      "4 conv1_relu\n",
      "5 pool1_pad\n",
      "6 pool1_pool\n",
      "7 conv2_block1_1_conv\n",
      "8 conv2_block1_1_bn\n",
      "9 conv2_block1_1_relu\n",
      "10 conv2_block1_2_conv\n",
      "11 conv2_block1_2_bn\n",
      "12 conv2_block1_2_relu\n",
      "13 conv2_block1_0_conv\n",
      "14 conv2_block1_3_conv\n",
      "15 conv2_block1_0_bn\n",
      "16 conv2_block1_3_bn\n",
      "17 conv2_block1_add\n",
      "18 conv2_block1_out\n",
      "19 conv2_block2_1_conv\n",
      "20 conv2_block2_1_bn\n",
      "21 conv2_block2_1_relu\n",
      "22 conv2_block2_2_conv\n",
      "23 conv2_block2_2_bn\n",
      "24 conv2_block2_2_relu\n",
      "25 conv2_block2_3_conv\n",
      "26 conv2_block2_3_bn\n",
      "27 conv2_block2_add\n",
      "28 conv2_block2_out\n",
      "29 conv2_block3_1_conv\n",
      "30 conv2_block3_1_bn\n",
      "31 conv2_block3_1_relu\n",
      "32 conv2_block3_2_conv\n",
      "33 conv2_block3_2_bn\n",
      "34 conv2_block3_2_relu\n",
      "35 conv2_block3_3_conv\n",
      "36 conv2_block3_3_bn\n",
      "37 conv2_block3_add\n",
      "38 conv2_block3_out\n",
      "39 conv3_block1_1_conv\n",
      "40 conv3_block1_1_bn\n",
      "41 conv3_block1_1_relu\n",
      "42 conv3_block1_2_conv\n",
      "43 conv3_block1_2_bn\n",
      "44 conv3_block1_2_relu\n",
      "45 conv3_block1_0_conv\n",
      "46 conv3_block1_3_conv\n",
      "47 conv3_block1_0_bn\n",
      "48 conv3_block1_3_bn\n",
      "49 conv3_block1_add\n",
      "50 conv3_block1_out\n",
      "51 conv3_block2_1_conv\n",
      "52 conv3_block2_1_bn\n",
      "53 conv3_block2_1_relu\n",
      "54 conv3_block2_2_conv\n",
      "55 conv3_block2_2_bn\n",
      "56 conv3_block2_2_relu\n",
      "57 conv3_block2_3_conv\n",
      "58 conv3_block2_3_bn\n",
      "59 conv3_block2_add\n",
      "60 conv3_block2_out\n",
      "61 conv3_block3_1_conv\n",
      "62 conv3_block3_1_bn\n",
      "63 conv3_block3_1_relu\n",
      "64 conv3_block3_2_conv\n",
      "65 conv3_block3_2_bn\n",
      "66 conv3_block3_2_relu\n",
      "67 conv3_block3_3_conv\n",
      "68 conv3_block3_3_bn\n",
      "69 conv3_block3_add\n",
      "70 conv3_block3_out\n",
      "71 conv3_block4_1_conv\n",
      "72 conv3_block4_1_bn\n",
      "73 conv3_block4_1_relu\n",
      "74 conv3_block4_2_conv\n",
      "75 conv3_block4_2_bn\n",
      "76 conv3_block4_2_relu\n",
      "77 conv3_block4_3_conv\n",
      "78 conv3_block4_3_bn\n",
      "79 conv3_block4_add\n",
      "80 conv3_block4_out\n",
      "81 conv4_block1_1_conv\n",
      "82 conv4_block1_1_bn\n",
      "83 conv4_block1_1_relu\n",
      "84 conv4_block1_2_conv\n",
      "85 conv4_block1_2_bn\n",
      "86 conv4_block1_2_relu\n",
      "87 conv4_block1_0_conv\n",
      "88 conv4_block1_3_conv\n",
      "89 conv4_block1_0_bn\n",
      "90 conv4_block1_3_bn\n",
      "91 conv4_block1_add\n",
      "92 conv4_block1_out\n",
      "93 conv4_block2_1_conv\n",
      "94 conv4_block2_1_bn\n",
      "95 conv4_block2_1_relu\n",
      "96 conv4_block2_2_conv\n",
      "97 conv4_block2_2_bn\n",
      "98 conv4_block2_2_relu\n",
      "99 conv4_block2_3_conv\n",
      "100 conv4_block2_3_bn\n",
      "101 conv4_block2_add\n",
      "102 conv4_block2_out\n",
      "103 conv4_block3_1_conv\n",
      "104 conv4_block3_1_bn\n",
      "105 conv4_block3_1_relu\n",
      "106 conv4_block3_2_conv\n",
      "107 conv4_block3_2_bn\n",
      "108 conv4_block3_2_relu\n",
      "109 conv4_block3_3_conv\n",
      "110 conv4_block3_3_bn\n",
      "111 conv4_block3_add\n",
      "112 conv4_block3_out\n",
      "113 conv4_block4_1_conv\n",
      "114 conv4_block4_1_bn\n",
      "115 conv4_block4_1_relu\n",
      "116 conv4_block4_2_conv\n",
      "117 conv4_block4_2_bn\n",
      "118 conv4_block4_2_relu\n",
      "119 conv4_block4_3_conv\n",
      "120 conv4_block4_3_bn\n",
      "121 conv4_block4_add\n",
      "122 conv4_block4_out\n",
      "123 conv4_block5_1_conv\n",
      "124 conv4_block5_1_bn\n",
      "125 conv4_block5_1_relu\n",
      "126 conv4_block5_2_conv\n",
      "127 conv4_block5_2_bn\n",
      "128 conv4_block5_2_relu\n",
      "129 conv4_block5_3_conv\n",
      "130 conv4_block5_3_bn\n",
      "131 conv4_block5_add\n",
      "132 conv4_block5_out\n",
      "133 conv4_block6_1_conv\n",
      "134 conv4_block6_1_bn\n",
      "135 conv4_block6_1_relu\n",
      "136 conv4_block6_2_conv\n",
      "137 conv4_block6_2_bn\n",
      "138 conv4_block6_2_relu\n",
      "139 conv4_block6_3_conv\n",
      "140 conv4_block6_3_bn\n",
      "141 conv4_block6_add\n",
      "142 conv4_block6_out\n",
      "143 conv5_block1_1_conv\n",
      "144 conv5_block1_1_bn\n",
      "145 conv5_block1_1_relu\n",
      "146 conv5_block1_2_conv\n",
      "147 conv5_block1_2_bn\n",
      "148 conv5_block1_2_relu\n",
      "149 conv5_block1_0_conv\n",
      "150 conv5_block1_3_conv\n",
      "151 conv5_block1_0_bn\n",
      "152 conv5_block1_3_bn\n",
      "153 conv5_block1_add\n",
      "154 conv5_block1_out\n",
      "155 conv5_block2_1_conv\n",
      "156 conv5_block2_1_bn\n",
      "157 conv5_block2_1_relu\n",
      "158 conv5_block2_2_conv\n",
      "159 conv5_block2_2_bn\n",
      "160 conv5_block2_2_relu\n",
      "161 conv5_block2_3_conv\n",
      "162 conv5_block2_3_bn\n",
      "163 conv5_block2_add\n",
      "164 conv5_block2_out\n",
      "165 conv5_block3_1_conv\n",
      "166 conv5_block3_1_bn\n",
      "167 conv5_block3_1_relu\n",
      "168 conv5_block3_2_conv\n",
      "169 conv5_block3_2_bn\n",
      "170 conv5_block3_2_relu\n",
      "171 conv5_block3_3_conv\n",
      "172 conv5_block3_3_bn\n",
      "173 conv5_block3_add\n",
      "174 conv5_block3_out\n",
      "175 global_average_pooling2d\n",
      "176 dense_4\n",
      "177 dense_5\n",
      "178 dense_6\n",
      "179 dense_7\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(image_only_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in image_only_model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "for layer in image_only_model.layers[:155]:\n",
    "    layer.trainable=False\n",
    "for layer in image_only_model.layers[155:]:\n",
    "    layer.trainable=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1617 images belonging to 2 classes.\n",
      "Found 663 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "test_generator=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_directory('D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\Troll_meme\\\\train',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True)\n",
    "test_generator = test_generator.flow_from_directory('D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\Troll_meme\\\\test',\n",
    "                                                 target_size=(224,224),\n",
    "                                                  color_mode='rgb',\n",
    "                                                  batch_size=1,\n",
    "                                                  class_mode='binary',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing imbalanced data with class weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_generator.classes),\n",
    "                                                 train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.21396396, 0.85015773])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "50/50 [==============================] - 223s 4s/step - loss: 0.3000 - f1_m: 0.8987\n",
      "Epoch 2/7\n",
      "50/50 [==============================] - 220s 4s/step - loss: 0.0669 - f1_m: 0.9821\n",
      "Epoch 3/7\n",
      "50/50 [==============================] - 219s 4s/step - loss: 0.0363 - f1_m: 0.9890\n",
      "Epoch 4/7\n",
      "50/50 [==============================] - 259s 5s/step - loss: 0.0134 - f1_m: 0.9970\n",
      "Epoch 5/7\n",
      "50/50 [==============================] - 275s 6s/step - loss: 0.0277 - f1_m: 0.9936\n",
      "Epoch 6/7\n",
      "50/50 [==============================] - 243s 5s/step - loss: 0.0190 - f1_m: 0.9958\n",
      "Epoch 7/7\n",
      "50/50 [==============================] - 249s 5s/step - loss: 0.0090 - f1_m: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20de14ce588>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_only_model.compile(optimizer=adam,loss='binary_crossentropy',metrics=[f1_m])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "image_only_model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "3WNiNVM6WiBq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[179 163]\n",
      " [158 163]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       troll       0.53      0.52      0.53       342\n",
      "   Not_troll       0.50      0.51      0.50       321\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       663\n",
      "   macro avg       0.52      0.52      0.52       663\n",
      "weighted avg       0.52      0.52      0.52       663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = image_only_model.predict_generator(test_generator, steps=663)\n",
    "y_pred = np.round(list(itertools.chain(*Y_pred)))\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['troll', 'Not_troll']\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [ int(x) for x in Y_pred ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text_df = pd.read_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\submit1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0;\n",
    "for i in Y_pred:\n",
    "    if i == 0:\n",
    "      only_text_df['label'][count] = \"troll\";\n",
    "    else:\n",
    "        only_text_df['label'][count] = \"not_troll\";\n",
    "    count = count + 1;\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0KH3Et2bIW5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "hWpvaSljbIT8"
   },
   "outputs": [],
   "source": [
    "only_text_df.to_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\my_submit1_img_resnet.csv', index = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnMG2rIgbINe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7mqxX1ObIKn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKjJoDqObIEl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_array(img_dirct):\n",
    "    all_imgs = []\n",
    "    for root, j, files in os.walk(img_dirct):\n",
    "        for file in files:\n",
    "            file = root + '/' + file\n",
    "            all_imgs.append(file)\n",
    "    return all_imgs\n",
    "\n",
    "def create_img_path(DF, Col_name, img_dir):\n",
    "    img_path = [img_dir + '/' + name for name in DF[Col_name]]\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "SPliz1IEbIBd"
   },
   "outputs": [],
   "source": [
    "# Processing image \n",
    "img_dir = \"D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\uploaded_tamil_memes\"\n",
    "\n",
    "img_dir_test = \"D:\\\\My Research\\\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\test_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = create_img_path(training_df,'image_name', img_dir)\n",
    "test_img_path = create_img_path(testing_df,'image_name', img_dir_test)\n",
    "val_img_path = create_img_path(validation_df,'image_name', img_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_text_generator(files, padded_seq, y, batch_size=None):\n",
    "    \"\"\"\n",
    "        padded_seq: vectorized padded text sequence \n",
    "        y: label of the text\n",
    "        batch_size: Number of observations to be selected at a time\n",
    "        \n",
    "        return: generator object of text data\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_idxs = np.random.choice(a = list(range(len(padded_seq))), size=batch_size) #Selecting the random batch indexes    \n",
    "        batch_input_txt = [] # Initializing batch input text\n",
    "        batch_input_img = [] # Initializing batch input image\n",
    "        batch_output = [] # Initializing batch output\n",
    "        \n",
    "        # Traversing through the batch indexes\n",
    "        for batch_idx in batch_idxs:\n",
    "            input_txt = padded_seq[batch_idx] # selecting padded sequences from the batch\n",
    "            output = y[batch_idx] # Selecting label  \n",
    "            input_img = get_input(files[batch_idx])\n",
    "            input_img = process_input(input_img)\n",
    "            batch_input_txt.append(input_txt) # Appending the input (text vector)\n",
    "            batch_input_img.append(input_img[0])\n",
    "            batch_output.append(output) # Appending the label\n",
    "        \n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x1 = np.array( batch_input_img )\n",
    "        batch_x2 = np.array( batch_input_txt )\n",
    "        batch_y = np.array( batch_output )\n",
    "        yield ([batch_x1, batch_x2], batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating train, test, val, generator for meme\n",
    "img_txt_gen_train = img_text_generator(train_img_path, x_train, y_train, batch_size=32)\n",
    "img_txt_gen_test = img_text_generator(test_img_path, x_test, y_test, batch_size=1)\n",
    "img_txt_gen_val = img_text_generator(val_img_path, x_val, y_val, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "IA5PMMtZ2qBx"
   },
   "outputs": [],
   "source": [
    "#Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "oA90m73b2p7_"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_m])\n",
    "image_only_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "_HUYD1tm2p30"
   },
   "outputs": [],
   "source": [
    "#concatenating the output of both the classifiers(text and image)\n",
    "con_layer = keras.layers.concatenate([model.output, image_only_model.output])\n",
    "out = Dense(1, activation='sigmoid')(con_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "nQ6rITNe2pxk"
   },
   "outputs": [],
   "source": [
    "#Common Model\n",
    "com_model = Model(inputs = [image_only_model.input, model.input], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "cI7XYjpg2puj"
   },
   "outputs": [],
   "source": [
    "\n",
    "com_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "cKTlpe4VchJB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 47s 37s/step - loss: 1.0897 - f1_m: 0.0000e+00 - val_loss: 0.9615 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 38s 32s/step - loss: 1.0998 - f1_m: 0.0000e+00 - val_loss: 1.0453 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 39s 34s/step - loss: 0.9967 - f1_m: 0.0000e+00 - val_loss: 0.9745 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 39s 33s/step - loss: 0.9478 - f1_m: 0.0000e+00 - val_loss: 0.9190 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 38s 33s/step - loss: 1.0102 - f1_m: 0.0000e+00 - val_loss: 0.8391 - val_f1_m: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 36s 31s/step - loss: 0.9890 - f1_m: 0.0000e+00 - val_loss: 0.8324 - val_f1_m: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 38s 33s/step - loss: 0.8963 - f1_m: 0.0000e+00 - val_loss: 0.7544 - val_f1_m: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 37s 32s/step - loss: 0.7823 - f1_m: 0.0000e+00 - val_loss: 0.7556 - val_f1_m: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 42s 37s/step - loss: 0.7703 - f1_m: 0.0000e+00 - val_loss: 0.7683 - val_f1_m: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 37s 32s/step - loss: 0.7876 - f1_m: 0.0000e+00 - val_loss: 0.7739 - val_f1_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#Training the combined model\n",
    "model3 = com_model.fit_generator(img_txt_gen_train, epochs=10, validation_steps = 150, steps_per_epoch=2, validation_data=img_txt_gen_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "FHU5sqMOcr6k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "com_model.save('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\LSTM_Resnet_Multimodl_model.h5')   # Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = com_model.predict_generator(img_txt_gen_test, steps=663)\n",
    "y_pred = np.round(list(itertools.chain(*Y_pred)))\n",
    "\n",
    "Y_pred = [ int(x) for x in Y_pred ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n"
     ]
    }
   ],
   "source": [
    "only_text_df = pd.read_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\submit1.csv')\n",
    "\n",
    "count = 0;\n",
    "for i in y_pred_txt:\n",
    "    if i == 0:\n",
    "      only_text_df['label'][count] = \"troll\";\n",
    "    else:\n",
    "        only_text_df['label'][count] = \"not_troll\";\n",
    "    count = count + 1;\n",
    "print(count);\n",
    "\n",
    "only_text_df.to_csv('D:\\\\My Research\\SHared Task\\\\Troll Meme\\\\Tamil_troll_memes\\\\tamil_memes_modified_Nusrat\\\\my_submit1_combined.csv', index = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJyOwBjHdSie"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "9PVWYMBpdSfV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.71939617395401, 0.009049772284924984]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating combined model by calculating loss and accuracy using test file\n",
    "com_model.evaluate_generator(img_txt_gen_test, steps=663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "teV5eNIFdrBY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maruf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8385684490203857, 0.0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text model loss and accuracy using test file\n",
    "model.evaluate_generator(txt_gen_test, steps=663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tScesdndgPbE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "ljdB96PKpxMZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-45aab054027f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plotting validation accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_f1_m'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_f1_m'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_f1_m'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting validation accuracy\n",
    "plt.plot(model3.history['val_f1_m'],'g')\n",
    "plt.plot(model1.history['val_f1_m'],'b')\n",
    "plt.plot(model2.history['val_f1_m'],'r')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.gca().legend(('meme model f1-score', 'text model f1-score', 'image model f1-score'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM+VGG16_(Text + Image+Combine).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
